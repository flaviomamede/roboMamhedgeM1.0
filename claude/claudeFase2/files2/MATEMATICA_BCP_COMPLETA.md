# ğŸ“ MATEMÃTICA COMPLETA DO BAYESIAN CHANGE POINT DETECTION

## ExplicaÃ§Ã£o Detalhada da Metodologia de Setz & WÃ¼rtz (ETH Zurich)

Este documento apresenta **toda a matemÃ¡tica** por trÃ¡s do algoritmo BCP implementado, com derivaÃ§Ãµes completas, intuiÃ§Ã£o e exemplos numÃ©ricos.

---

## ğŸ“š ÃNDICE

1. [Fundamentos Conceituais](#1-fundamentos-conceituais)
2. [Product Partition Model (PPM)](#2-product-partition-model-ppm)
3. [Priors Bayesianos](#3-priors-bayesianos)
4. [Likelihood e Data Factors](#4-likelihood-e-data-factors)
5. [Posterior Distributions](#5-posterior-distributions)
6. [Algoritmo Forward-Backward](#6-algoritmo-forward-backward)
7. [CÃ¡lculo de Probabilidades](#7-cÃ¡lculo-de-probabilidades)
8. [ImplementaÃ§Ã£o PrÃ¡tica](#8-implementaÃ§Ã£o-prÃ¡tica)
9. [Exemplo NumÃ©rico Completo](#9-exemplo-numÃ©rico-completo)

---

## 1. FUNDAMENTOS CONCEITUAIS

### 1.1 O Problema de Change Point

**Objetivo:** Dado uma sÃ©rie temporal X = {Xâ‚, Xâ‚‚, ..., Xâ‚™}, queremos identificar pontos onde a **estrutura geradora** dos dados muda.

**DefiniÃ§Ã£o Formal:**

Existe uma **partiÃ§Ã£o** Ï = (iâ‚€, iâ‚, ..., iáµ¦) tal que:

```
0 = iâ‚€ < iâ‚ < iâ‚‚ < ... < iáµ¦ = n
```

E dentro de cada **bloco** [iâ‚–â‚‹â‚ + 1, iâ‚–], os dados sÃ£o i.i.d. (independentes e identicamente distribuÃ­dos) com parÃ¢metros Î¸áµ¢â‚–.

**Exemplo Visual:**

```
X: [2.1, 2.3, 2.0, 2.2 | 5.1, 5.3, 5.0, 5.2 | 1.9, 2.1, 2.0]
          Bloco 1        |     Bloco 2        |     Bloco 3
       Î¼â‚ â‰ˆ 2.15         |   Î¼â‚‚ â‰ˆ 5.15        |   Î¼â‚ƒ â‰ˆ 2.0
   Change point em iâ‚=4  |  Change point em iâ‚‚=8
```

### 1.2 Abordagem Bayesiana

Na abordagem Bayesiana, **tudo** Ã© uma distribuiÃ§Ã£o de probabilidade:

1. **Prior:** P(Ï) - nossa crenÃ§a a priori sobre partiÃ§Ãµes
2. **Likelihood:** P(X|Ï, Î¸) - probabilidade dos dados dada uma partiÃ§Ã£o
3. **Posterior:** P(Ï|X) - nossa crenÃ§a a posteriori sobre partiÃ§Ãµes

**Teorema de Bayes:**

```
P(Ï|X) = P(X|Ï) Â· P(Ï) / P(X)
```

Onde:
- P(X|Ï) = âˆ« P(X|Ï, Î¸) Â· P(Î¸|Ï) dÎ¸ (marginalizando sobre parÃ¢metros)
- P(X) = Î£áµ¨ P(X|Ï) Â· P(Ï) (evidÃªncia total)

---

## 2. PRODUCT PARTITION MODEL (PPM)

### 2.1 DefiniÃ§Ã£o do PPM

O **Product Partition Model** assume que:

**1. IndependÃªncia entre blocos:**
```
P(X|Ï, Î¸) = âˆ P(X_{iâ‚–â‚‹â‚+1:iâ‚–} | Î¸áµ¢â‚–)
           k=1..b
```

**2. Prior fatorÃ¡vel:**
```
P(Î¸|Ï) = âˆ P(Î¸áµ¢â‚–)
        k=1..b
```

**3. Cohesions (coesÃµes):**

A probabilidade prior de uma partiÃ§Ã£o Ã© proporcional ao produto de **cohesions** cáµ¢â±¼:

```
P(Ï) âˆ âˆ c_{iâ‚–â‚‹â‚,iâ‚–}
       k=1..b
```

Onde cáµ¢â±¼ mede a "coesÃ£o" (probabilidade a priori) do bloco [i+1, j].

### 2.2 Data Factors

O **data factor** fáµ¢â±¼ Ã© a verossimilhanÃ§a marginal do bloco [i+1, j]:

```
fáµ¢â±¼(Xáµ¢â±¼) = âˆ« P(Xáµ¢â±¼|Î¸) Â· P(Î¸) dÎ¸
```

Esta integral **elimina** Î¸, deixando apenas os dados observados.

**Propriedade Fundamental:**

```
P(X|Ï) = âˆ fáµ¢â±¼(Xáµ¢â±¼)
        ijâˆˆÏ
```

### 2.3 RelevÃ¢ncias (Relevances)

A **relevÃ¢ncia** ráµ¢â±¼ Ã© a probabilidade posterior de que o bloco [i+1, j] aparece em alguma partiÃ§Ã£o:

```
ráµ¢â±¼(X) = P(bloco [i+1, j] estÃ¡ em Ï | X)
```

**CÃ¡lculo via Î»-recursÃ£o:**

Defina Î»áµ¢â±¼ como a soma de produtos sobre todas partiÃ§Ãµes de [i+1, j]:

```
Î»áµ¢â±¼ = Î£  âˆ cáµ¢â‚–â‚‹â‚,áµ¢â‚–
     Ï k=1..b
```

EntÃ£o:

```
ráµ¢â±¼(X) = (Î»â‚€áµ¢ Â· cÌƒáµ¢â±¼ Â· Î»â±¼â‚™) / Î»â‚€â‚™
```

Onde cÌƒáµ¢â±¼ = cáµ¢â±¼ Â· fáµ¢â±¼(Xáµ¢â±¼) Ã© a **posterior cohesion**.

---

## 3. PRIORS BAYESIANOS

### 3.1 Prior sobre PartiÃ§Ãµes (Cohesions)

**Modelo GeomÃ©trico de Barry & Hartigan:**

```
cáµ¢â±¼ = p Â· (1-p)^(j-i-1)    para j < n
cáµ¢â‚™ = (1-p)^(n-i-1)         para j = n
```

Onde p âˆˆ (0,1] Ã© a **probabilidade de mudanÃ§a** por observaÃ§Ã£o.

**IntuiÃ§Ã£o:**
- p alto â†’ muitas mudanÃ§as esperadas
- p baixo â†’ poucas mudanÃ§as esperadas

**InterpretaÃ§Ã£o ProbabilÃ­stica:**

A cada passo, hÃ¡:
- Probabilidade p de ter um change point
- Probabilidade (1-p) de continuar no mesmo regime

### 3.2 Prior sobre ParÃ¢metros (Modelo Normal)

O BCP assume que dentro de cada bloco:

```
Xáµ¢ ~ N(Î¼, ÏƒÂ²)    (observaÃ§Ãµes)
Î¼  ~ N(Î¼â‚€, Ïƒâ‚€Â²/(j-i))   (prior conjugado)
```

**MotivaÃ§Ã£o do Prior:**

- Blocos **longos** â†’ pequeno desvio de Î¼â‚€ esperado â†’ prior tight
- Blocos **curtos** â†’ grande desvio de Î¼â‚€ possÃ­vel â†’ prior wide

Isto faz sentido porque Ã© difÃ­cil detectar pequenas mudanÃ§as em blocos curtos!

### 3.3 Hiperpriors (Full Bayesian Approach)

No modelo completo de Setz, usamos hiperpriors:

```
P(Î¼â‚€) = 1                     (improper, -âˆ < Î¼â‚€ < âˆ)
P(ÏƒÂ²) = 1/ÏƒÂ²                  (Jeffreys prior, ÏƒÂ² > 0)
P(p) = 1/pâ‚€                   (uniforme em [0, pâ‚€])
P(w) = 1/wâ‚€                   (uniforme em [0, wâ‚€])
```

Onde w = ÏƒÂ²/(Ïƒâ‚€Â² + ÏƒÂ²) Ã© a **razÃ£o de variÃ¢ncias**.

**InvariÃ¢ncias:**
- Invariante a translaÃ§Ãµes (Î¼â‚€)
- Invariante a escala (ÏƒÂ²)

---

## 4. LIKELIHOOD E DATA FACTORS

### 4.1 Likelihood Condicional

Dado um bloco [i+1, j] com parÃ¢metros (Î¼, ÏƒÂ²):

```
P(Xáµ¢â±¼ | Î¼, ÏƒÂ²) = âˆ (1/âˆš(2Ï€ÏƒÂ²)) Â· exp(-(Xâ‚— - Î¼)Â²/(2ÏƒÂ²))
                l=i+1..j

               = (2Ï€ÏƒÂ²)^(-(j-i)/2) Â· exp(-Î£(Xâ‚— - Î¼)Â²/(2ÏƒÂ²))
```

### 4.2 Data Factor (Integrando Î¼)

**Prior:** Î¼ ~ N(Î¼â‚€, Ïƒâ‚€Â²/(j-i))

**Posterior (conjugado):**

```
Î¼ | Xáµ¢â±¼, ÏƒÂ² ~ N(Î¼Ì‚áµ¢â±¼, ÏƒÌ‚Â²áµ¢â±¼)
```

Onde:

```
ÏƒÌ‚Â²áµ¢â±¼ = 1 / (1/Ïƒâ‚€Â² + (j-i)/ÏƒÂ²)
Î¼Ì‚áµ¢â±¼ = ÏƒÌ‚Â²áµ¢â±¼ Â· (Î¼â‚€/Ïƒâ‚€Â² + (j-i)Â·XÌ„áµ¢â±¼/ÏƒÂ²)
```

**Data Factor (integrando Î¼):**

```
fáµ¢â±¼(Xáµ¢â±¼ | ÏƒÂ²) = (2Ï€ÏƒÂ²)^(-(j-i)/2) Â· (ÏƒÂ²/(Ïƒâ‚€Â² + ÏƒÂ²))^(1/2) Â· exp(Váµ¢â±¼)
```

Onde:

```
Váµ¢â±¼ = -Î£(Xâ‚— - XÌ„áµ¢â±¼)Â²/(2ÏƒÂ²) - (j-i)(XÌ„áµ¢â±¼ - Î¼â‚€)Â²/(2(Ïƒâ‚€Â² + ÏƒÂ²))
```

**Componentes de Váµ¢â±¼:**

1. **W_ij = Î£(Xâ‚— - XÌ„áµ¢â±¼)Â²:** VariaÃ§Ã£o **dentro** do bloco (within-block variance)
2. **B_ij = (j-i)(XÌ„áµ¢â±¼ - Î¼â‚€)Â²:** VariaÃ§Ã£o **entre** blocos (between-block variance)

### 4.3 Data Factor (Integrando Î¼ e ÏƒÂ²)

**Prior:** ÏƒÂ² ~ 1/ÏƒÂ² (improper Jeffreys)

Integrando sobre ÏƒÂ²:

```
fáµ¢â±¼(Xáµ¢â±¼) = âˆ«â‚€^âˆ fáµ¢â±¼(Xáµ¢â±¼ | ÏƒÂ²) Â· (1/ÏƒÂ²) dÏƒÂ²
```

**Resultado (usando improper priors):**

```
fáµ¢â±¼(Xáµ¢â±¼) âˆ âˆ«â‚€^wâ‚€ w^((b-1)/2) / (W + Bw)^((n-1)/2) dw
```

Esta integral Ã© uma **incomplete beta function** que pode ser calculada numericamente.

### 4.4 AproximaÃ§Ã£o Simplificada

Na implementaÃ§Ã£o prÃ¡tica (versÃ£o rÃ¡pida), usamos:

```
log fáµ¢â±¼ â‰ˆ -(j-i-1)/2 Â· log(W_ij + Îµ)
```

Onde:
- W_ij = Î£(Xâ‚— - XÌ„áµ¢â±¼)Â² Ã© a variÃ¢ncia dentro do bloco
- Îµ > 0 Ã© um termo de regularizaÃ§Ã£o pequeno

**IntuiÃ§Ã£o:** Blocos com **baixa variÃ¢ncia interna** tÃªm **alta likelihood**.

---

## 5. POSTERIOR DISTRIBUTIONS

### 5.1 Posterior sobre PartiÃ§Ãµes

**Teorema de Bayes:**

```
P(Ï|X) = P(X|Ï) Â· P(Ï) / P(X)
       = [âˆ fáµ¢â±¼(Xáµ¢â±¼)] Â· [âˆ cáµ¢â±¼] / P(X)
       = [âˆ cÌƒáµ¢â±¼] / P(X)
```

Onde cÌƒáµ¢â±¼ = cáµ¢â±¼ Â· fáµ¢â±¼ Ã© a **posterior cohesion**.

### 5.2 Posterior sobre ParÃ¢metros

Para um bloco [i+1, j], o parÃ¢metro posterior Ã©:

```
Î¼áµ¢â±¼ | Xáµ¢â±¼ ~ N(Î¼Ì‚áµ¢â±¼, ÏƒÌ‚Â²áµ¢â±¼)
```

Com:

```
Î¼Ì‚áµ¢â±¼ = (1-w)Â·XÌ„áµ¢â±¼ + wÂ·Î¼â‚€
```

Onde w = ÏƒÂ²/(Ïƒâ‚€Â² + ÏƒÂ²) Ã© o **peso do prior**.

**IntuiÃ§Ã£o:**
- w â†’ 0: Prior fraco, Î¼Ì‚áµ¢â±¼ â‰ˆ XÌ„áµ¢â±¼ (acredita nos dados)
- w â†’ 1: Prior forte, Î¼Ì‚áµ¢â±¼ â‰ˆ Î¼â‚€ (acredita no prior)

### 5.3 MarginalizaÃ§Ã£o sobre PartiÃ§Ãµes

A **mÃ©dia posterior final** em cada ponto k Ã©:

```
E[Î¼â‚– | X] = Î£  E[Î¼â‚– | Xáµ¢â±¼, Ï] Â· P(Ï | X)
           Ï
```

Esta Ã© uma **mÃ©dia ponderada** sobre TODAS as partiÃ§Ãµes possÃ­veis!

**Usando relevÃ¢ncias:**

```
E[Î¼â‚– | X] = Î£  E[Î¼â‚– | Xáµ¢â±¼] Â· ráµ¢â±¼(X)
          i<kâ‰¤j
```

Onde a soma Ã© sobre todos blocos que **contÃªm** k.

---

## 6. ALGORITMO FORWARD-BACKWARD

### 6.1 Problema Computacional

EnumeraÃ§Ã£o exaustiva:
- NÃºmero de partiÃ§Ãµes de n elementos = **NÃºmero de Bell** Bâ‚™
- Bâ‚â‚€ = 115,975
- Bâ‚‚â‚€ = 51,724,158,235,372
- Bâ‚…â‚€ â‰ˆ 10^47 (intratÃ¡vel!)

**SoluÃ§Ã£o:** ProgramaÃ§Ã£o dinÃ¢mica usando **Product Partition Model**.

### 6.2 Lambda-RecursÃ£o (Forward)

**DefiniÃ§Ã£o:**

Î»áµ¢â±¼ = soma sobre todas partiÃ§Ãµes de [i+1, j]

**RecursÃ£o:**

```
Î»áµ¢â±¼ = Î£  cáµ¢â‚– Â· Î»â‚–â±¼
     k=i..j-1
```

**Caso Base:**
```
Î»áµ¢áµ¢ = 1    (partiÃ§Ã£o vazia)
```

**Complexidade:** O(nÂ²)

### 6.3 Backward Pass (RelevÃ¢ncias)

A relevÃ¢ncia ráµ¢â±¼ pode ser calculada como:

```
ráµ¢â±¼(X) = (Î»â‚€áµ¢ Â· cÌƒáµ¢â±¼ Â· Î»â±¼â‚™) / Î»â‚€â‚™
```

**Algoritmo:**

```
1. Forward: Calcular Î»â‚€â±¼ para j = 1..n (probabilidade de chegar em j)
2. Backward: Calcular Î»â±¼â‚™ para j = 0..n-1 (probabilidade de j atÃ© n)
3. Relevance: ráµ¢â±¼ = (Î»â‚€áµ¢ Â· cÌƒáµ¢â±¼ Â· Î»â±¼â‚™) / Î»â‚€â‚™
```

### 6.4 Probabilidade de Change Point

A probabilidade de haver mudanÃ§a no ponto t:

```
P(change em t | X) = Î£ P(Ï | X)
                    Ï: t Ã© change point em Ï
```

**CÃ¡lculo via Forward-Backward:**

```
P(change em t | X) = (Î»â‚€,â‚œâ‚‹â‚ Â· p Â· Î»â‚œâ‚™) / Î»â‚€â‚™
```

Onde:
- Î»â‚€,â‚œâ‚‹â‚: probabilidade forward atÃ© t-1
- p: prior de mudanÃ§a
- Î»â‚œâ‚™: probabilidade backward de t atÃ© n
- Î»â‚€â‚™: normalizaÃ§Ã£o

---

## 7. CÃLCULO DE PROBABILIDADES

### 7.1 Log-Space Arithmetic

Para evitar **underflow numÃ©rico**, trabalhamos em log-space:

```
log(a + b) = log(a) + log(1 + exp(log(b) - log(a)))
           = logsumexp([log(a), log(b)])
```

**Python:**
```python
from scipy.special import logsumexp
log_sum = logsumexp([log_a, log_b])
```

### 7.2 FÃ³rmula PrÃ¡tica do Forward

```
log Î»â‚€â±¼ = logsumexp([
    log Î»â‚€áµ¢ + log cáµ¢â±¼ + log fáµ¢â±¼
    for i in range(j)
])
```

### 7.3 ConversÃ£o para Probabilidades

```
P(change em t) = exp(log_Î»â‚€,â‚œâ‚‹â‚ + log(p) + log_Î»â‚œâ‚™ - log_Î»â‚€â‚™)
```

**Clipping:** Limitar entre [0, 1] para evitar erros numÃ©ricos.

---

## 8. IMPLEMENTAÃ‡ÃƒO PRÃTICA

### 8.1 VersÃ£o Simplificada (Janela MÃ³vel)

Para sÃ©ries longas (n > 1000), usar janela mÃ³vel:

**Algoritmo:**

```
Para cada t = 1..n:
    1. Pegar janela [t - window, t]
    2. Detectar mudanÃ§as dentro da janela
    3. Atribuir probabilidade ao ponto t
```

**Vantagem:** Complexidade O(n Â· windowÂ²) ao invÃ©s de O(nÂ³).

### 8.2 Teste de MÃºltiplos Split Points

Dentro da janela, testar splits em posiÃ§Ãµes estratÃ©gicas:

```
split_positions = [0.2, 0.4, 0.6, 0.8] Â· window_size
```

Para cada split:

1. Calcular likelihood do modelo com split
2. Calcular likelihood do modelo sem split
3. Bayes Factor = likelihood_with / likelihood_without
4. Converter para probabilidade via prior

### 8.3 Bayes Factor

```
BF = P(dados | Hâ‚: hÃ¡ mudanÃ§a) / P(dados | Hâ‚€: nÃ£o hÃ¡ mudanÃ§a)
```

**InterpretaÃ§Ã£o:**
- BF > 10: EvidÃªncia forte para mudanÃ§a
- BF > 3: EvidÃªncia moderada
- BF < 1/3: EvidÃªncia contra mudanÃ§a

**ConversÃ£o para Probabilidade:**

```
P(mudanÃ§a | dados) = BF Â· P(mudanÃ§a) / (BF Â· P(mudanÃ§a) + 1 - P(mudanÃ§a))
```

---

## 9. EXEMPLO NUMÃ‰RICO COMPLETO

### 9.1 Setup

Dados simulados:
```
X = [2.0, 2.1, 2.0, 2.2, 5.0, 5.1, 5.0, 5.2]
      â†â”€â”€â”€ Regime 1 â”€â”€â”€â”€â†’  â†â”€â”€â”€ Regime 2 â”€â”€â”€â†’
```

ParÃ¢metros:
- p = 0.2 (prior de mudanÃ§a)
- Î¼â‚€ = 0.0 (prior da mÃ©dia)
- ÏƒÂ² = 1.0 (variÃ¢ncia conhecida)
- Ïƒâ‚€Â² = 10.0 (variÃ¢ncia do prior)

### 9.2 CÃ¡lculo dos Data Factors

**Bloco [1,4]:** X = [2.0, 2.1, 2.0, 2.2]
```
XÌ„â‚â‚„ = 2.075
Wâ‚â‚„ = Î£(Xáµ¢ - 2.075)Â² = 0.0075 + 0.000625 + 0.005625 + 0.015625 = 0.0294

log fâ‚â‚„ â‰ˆ -3/2 Â· log(0.0294) = 5.24
```

**Bloco [5,8]:** X = [5.0, 5.1, 5.0, 5.2]
```
XÌ„â‚…â‚ˆ = 5.075
Wâ‚…â‚ˆ = 0.0294   (mesma variÃ¢ncia interna!)

log fâ‚…â‚ˆ â‰ˆ 5.24
```

**Bloco [1,8]:** X = [2.0, 2.1, 2.0, 2.2, 5.0, 5.1, 5.0, 5.2]
```
XÌ„â‚â‚ˆ = 3.575
Wâ‚â‚ˆ = Î£(Xáµ¢ - 3.575)Â² = 4Â·(1.575)Â² + 4Â·(1.475)Â² = 18.41

log fâ‚â‚ˆ â‰ˆ -7/2 Â· log(18.41) = -10.13
```

### 9.3 Cohesions

```
câ‚â‚„ = 0.2 Â· (0.8)Â³ = 0.1024
câ‚…â‚ˆ = 0.2 Â· (0.8)Â³ = 0.1024
câ‚â‚ˆ = 0.2 Â· (0.8)â· = 0.0419
```

### 9.4 Posterior Cohesions

```
cÌƒâ‚â‚„ = câ‚â‚„ Â· fâ‚â‚„ = 0.1024 Â· exp(5.24) = 19.2
cÌƒâ‚…â‚ˆ = câ‚…â‚ˆ Â· fâ‚…â‚ˆ = 0.1024 Â· exp(5.24) = 19.2
cÌƒâ‚â‚ˆ = câ‚â‚ˆ Â· fâ‚â‚ˆ = 0.0419 Â· exp(-10.13) = 0.0000015
```

### 9.5 ComparaÃ§Ã£o de Modelos

**Modelo A:** MudanÃ§a em t=4
```
P(X | modelo A) âˆ cÌƒâ‚â‚„ Â· cÌƒâ‚…â‚ˆ = 19.2 Â· 19.2 = 368.64
```

**Modelo B:** Sem mudanÃ§a
```
P(X | modelo B) âˆ cÌƒâ‚â‚ˆ = 0.0000015
```

**Bayes Factor:**
```
BF = 368.64 / 0.0000015 â‰ˆ 2.5 Ã— 10â¸  (!!!)
```

**ConclusÃ£o:** EvidÃªncia **extremamente forte** de mudanÃ§a em t=4.

### 9.6 Probabilidade Posterior

```
prior_odds = p/(1-p) = 0.2/0.8 = 0.25
posterior_odds = BF Â· prior_odds = 2.5Ã—10â¸ Â· 0.25 = 6.25Ã—10â·

P(mudanÃ§a em t=4 | X) = posterior_odds / (1 + posterior_odds)
                       â‰ˆ 0.9999999984  â‰ˆ 100%
```

---

## 10. INTUIÃ‡ÃƒO E INSIGHTS

### 10.1 Por Que o BCP Funciona?

**1. ParsimÃ´nia Bayesiana:**
- Prior favorece poucas mudanÃ§as (via cohesions geomÃ©tricas)
- Dados superam prior apenas quando mudanÃ§a Ã© **clara**

**2. Automatic Relevance Determination:**
- Blocos curtos tÃªm prior largo â†’ tolera grandes desvios
- Blocos longos tÃªm prior apertado â†’ exige consistÃªncia

**3. MarginalizaÃ§Ã£o:**
- Considera TODAS partiÃ§Ãµes possÃ­veis
- Robusto a incerteza sobre localizaÃ§Ã£o exata

### 10.2 InterpretaÃ§Ã£o dos HiperparÃ¢metros

**p (probabilidade de mudanÃ§a):**
- p pequeno (0.05-0.15): Conservador, detecta apenas mudanÃ§as drÃ¡sticas
- p grande (0.25-0.40): Agressivo, detecta mudanÃ§as sutis

**w (razÃ£o de variÃ¢ncias):**
- w pequeno (0.05-0.15): Prior fraco, acredita mais nos dados
- w grande (0.25-0.40): Prior forte, requer mudanÃ§as maiores

### 10.3 LimitaÃ§Ãµes

**1. Assumem normalidade:**
- Se dados sÃ£o heavy-tailed, pode dar falsos positivos
- SoluÃ§Ã£o: Usar transformaÃ§Ãµes (log, rank)

**2. IndependÃªncia:**
- NÃ£o modela autocorrelaÃ§Ã£o
- Em sÃ©ries com forte dependÃªncia temporal, pode ser subÃ³timo

**3. Retrospectivo:**
- Resultado muda conforme chegam novos dados
- A probabilidade em t depende de observaÃ§Ãµes futuras!

### 10.4 ExtensÃµes AvanÃ§adas

**1. MudanÃ§a em mÃºltiplos parÃ¢metros:**
- Detectar mudanÃ§as em mÃ©dia **E** variÃ¢ncia simultaneamente
- Modelo N-NGIG de Setz (Normal - Normal Generalized Inverse Gaussian)

**2. Markov Dependency:**
- ParÃ¢metros de blocos adjacentes correlacionados
- Mais complexo, mas mais realista

**3. Online Detection:**
- Usar apenas dados atÃ© t para estimar P(change em t)
- Perde informaÃ§Ã£o, mas permite uso em tempo real

---

## 11. COMPARAÃ‡ÃƒO COM OUTROS MÃ‰TODOS

### 11.1 vs. CUSUM

**CUSUM (Cumulative Sum):**
```
Sâ‚œ = max(0, Sâ‚œâ‚‹â‚ + (Xâ‚œ - Î¼â‚€) - k)
```

**Vantagens BCP:**
- âœ… Quantifica incerteza (probabilidade)
- âœ… Detecta mÃºltiplas mudanÃ§as
- âœ… NÃ£o requer threshold manual

**Vantagens CUSUM:**
- âœ… Computacionalmente mais rÃ¡pido
- âœ… Controle direto de false alarm rate

### 11.2 vs. Structural Break Tests

**Chow Test, Bai-Perron:**
- Baseados em F-statistics
- Requerem especificaÃ§Ã£o do nÃºmero de breaks

**Vantagens BCP:**
- âœ… InferÃªncia automÃ¡tica de nÃºmero de mudanÃ§as
- âœ… Prior bayesiano incorpora conhecimento a priori
- âœ… Posterior distribution completa

### 11.3 vs. Hidden Markov Models

**HMM:**
- Assume estados latentes discretos
- TransiÃ§Ãµes entre estados via matriz

**Vantagens BCP:**
- âœ… NÃ£o requer especificar nÃºmero de estados
- âœ… Mais interpretÃ¡vel (mudanÃ§as pontuais)
- âœ… Menos parÃ¢metros a estimar

**Vantagens HMM:**
- âœ… Modela dependÃªncia temporal
- âœ… Permite transiÃ§Ãµes reversÃ­veis

---

## 12. CHECKLIST DE IMPLEMENTAÃ‡ÃƒO

### âœ… PrÃ©-processamento
- [ ] Remover outliers extremos (> 5Ïƒ)
- [ ] Verificar stationaridade (se muito nÃ£o-estacionÃ¡rio, diferenciar)
- [ ] Normalizar se escalas muito diferentes

### âœ… Escolha de HiperparÃ¢metros
- [ ] ComeÃ§ar com p = 0.2, w = 0.2
- [ ] Se muitos falsos positivos: reduzir p
- [ ] Se nÃ£o detecta mudanÃ§as Ã³bvias: aumentar p

### âœ… ValidaÃ§Ã£o
- [ ] Testar em dados sintÃ©ticos com mudanÃ§as conhecidas
- [ ] Verificar calibraÃ§Ã£o: P(change) vs taxa real
- [ ] Cross-validation em sÃ©ries similares

### âœ… InterpretaÃ§Ã£o
- [ ] Percentil > 90: Alerta mÃ¡ximo
- [ ] Percentil 75-90: Cautela
- [ ] Percentil < 60: Normal
- [ ] Combinar com outros indicadores

---

## 13. CÃ“DIGO MÃNIMO COMENTADO

```python
import numpy as np
from scipy.special import logsumexp

def bcp_detect(returns, p0=0.2, max_block=200):
    """
    BCP via Forward-Backward (implementaÃ§Ã£o correta).
    
    Args:
        returns: np.array de retornos
        p0: prior de probabilidade de mudanÃ§a
        max_block: tamanho mÃ¡ximo de bloco (limita complexidade)
    
    Returns:
        posterior_prob: P(change em t | dados) para cada t
    """
    n = len(returns)
    
    # Log-cohesions (prior geomÃ©trico de Barry & Hartigan)
    log_p = np.log(p0)
    log_1mp = np.log(1 - p0)
    
    def log_cohesion(block_len, is_last):
        """Cohesion: pÂ·(1-p)^(len-1) ou (1-p)^(len-1) para Ãºltimo bloco."""
        if is_last:
            return (block_len - 1) * log_1mp
        return log_p + (block_len - 1) * log_1mp
    
    # Data factors (log-likelihood marginal de cada bloco)
    log_factors = {}
    for i in range(n):
        for j in range(i+1, min(i + max_block + 1, n+1)):
            block = returns[i:j]
            if len(block) < 2:
                log_factors[(i,j)] = 0.0
            else:
                mu = np.mean(block)
                W = np.sum((block - mu)**2)
                log_factors[(i,j)] = -(len(block)-1)/2 * np.log(W + 1e-10)
    
    # Forward pass
    log_forward = np.full(n+1, -np.inf)
    log_forward[0] = 0.0
    
    for j in range(1, n+1):
        log_probs = []
        for i in range(max(0, j - max_block), j):
            log_coh = log_cohesion(j - i, j == n)
            if (i, j) in log_factors:
                log_probs.append(log_forward[i] + log_coh + log_factors[(i,j)])
        if log_probs:
            log_forward[j] = logsumexp(log_probs)
    
    # Backward pass (ESSENCIAL: computar separadamente!)
    log_backward = np.full(n+1, -np.inf)
    log_backward[n] = 0.0
    
    for j in range(n-1, -1, -1):
        log_probs = []
        for k in range(j+1, min(j + max_block + 1, n+1)):
            log_coh = log_cohesion(k - j, k == n)
            if (j, k) in log_factors:
                log_probs.append(log_coh + log_factors[(j,k)] + log_backward[k])
        if log_probs:
            log_backward[j] = logsumexp(log_probs)
    
    # Probabilidade de change point em t
    posterior_prob = np.zeros(n)
    for t in range(1, n):
        log_post = log_forward[t] + log_backward[t] - log_forward[n]
        posterior_prob[t] = np.clip(np.exp(log_post), 0, 1)
    
    return posterior_prob

# Uso:
# prob = bcp_detect(retornos_ibovespa, p0=0.18)
# print(f"Probabilidade atual: {prob[-1]:.2%}")
```

---

## 14. REFERÃŠNCIAS MATEMÃTICAS

### Papers Fundamentais

1. **Barry, D. & Hartigan, J.A. (1992)**
   "Product Partition Models for Change Point Problems"
   *Annals of Statistics*, 20(1), 260-279
   
2. **Barry, D. & Hartigan, J.A. (1993)**
   "A Bayesian Analysis for Change Point Problems"
   *Journal of the American Statistical Association*, 35, 309-319

3. **Setz, T. (2017)**
   "Stable Portfolio Design Using Bayesian Change Point Models and Geometric Shape Factors"
   *ETH Zurich PhD Thesis*
   DOI: 10.3929/ethz-b-000244960

### Livros Recomendados

4. **Gelman et al. (2013)**
   "Bayesian Data Analysis" (3rd ed.)
   Chapman & Hall/CRC

5. **Bishop, C.M. (2006)**
   "Pattern Recognition and Machine Learning"
   Springer (CapÃ­tulo sobre HMMs tem conexÃµes)

---

## ğŸ“ RESUMO EXECUTIVO

### O Que o BCP Faz?

Dado uma sÃ©rie Xâ‚, ..., Xâ‚™, calcula para cada ponto t:

```
P(mudanÃ§a estrutural em t | todos os dados observados)
```

### Como Funciona?

1. **Prior:** Assume poucas mudanÃ§as (via cohesions geomÃ©tricas)
2. **Likelihood:** Calcula probabilidade dos dados sob cada partiÃ§Ã£o possÃ­vel
3. **Posterior:** Combina via Bayes para obter probabilidades
4. **EficiÃªncia:** Usa programaÃ§Ã£o dinÃ¢mica (Forward-Backward) O(nÂ²)

### HiperparÃ¢metros Principais

- **p âˆˆ (0.1, 0.3):** Probabilidade a priori de mudanÃ§a por ponto
  - Menor = mais conservador
  - Maior = mais sensÃ­vel

### Output TÃ­pico

```
Tempo   PreÃ§o    P(change)   Status
t=100   125000   0.15        ğŸŸ¢ EstÃ¡vel
t=200   127000   0.42        ğŸŸ¡ AtenÃ§Ã£o
t=300   122000   0.87        ğŸ”´ Alerta!
```

### InterpretaÃ§Ã£o

- **P > 0.75:** Alta probabilidade de reversÃ£o
- **P âˆˆ [0.5, 0.75]:** Probabilidade moderada
- **P < 0.5:** Baixa probabilidade

---

**FIM DO DOCUMENTO MATEMÃTICO COMPLETO**

*Para dÃºvidas sobre implementaÃ§Ã£o especÃ­fica, consulte o cÃ³digo comentado na seÃ§Ã£o 13 ou os scripts Python fornecidos.*
